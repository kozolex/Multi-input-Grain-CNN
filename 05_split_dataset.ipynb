{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba ziaren w zbiorze treningowym: 69506 (obrazy: 208509)\n",
      "Liczba ziaren w zbiorze walidacyjnym: 14894 (obrazy: 44680)\n",
      "Liczba ziaren w zbiorze testowym: 14895 (obrazy: 44682)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def split_dataset(input_csv, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, random_state=42):\n",
    "    \"\"\"\n",
    "    Dzieli dane na zbiory treningowy, walidacyjny i testowy z zachowaniem proporcji klas i grupowania po ziarnach.\n",
    "\n",
    "    Args:\n",
    "        input_csv (str): Ścieżka do pliku CSV z danymi.\n",
    "        train_ratio (float): Proporcja danych treningowych.\n",
    "        val_ratio (float): Proporcja danych walidacyjnych.\n",
    "        test_ratio (float): Proporcja danych testowych.\n",
    "        random_state (int): Losowy seed dla powtarzalności.\n",
    "\n",
    "    Returns:\n",
    "        train_df, val_df, test_df: Dane podzielone na zbiory.\n",
    "    \"\"\"\n",
    "    # Wczytaj dane\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Grupowanie ziaren na podstawie unikalnego `id`\n",
    "    grouped = df.groupby('id').first()  # Wybieramy reprezentatywny wiersz dla każdego ziarna\n",
    "    ids = grouped.index\n",
    "    classes = grouped['class']\n",
    "\n",
    "    # Inicjalizacja StratifiedShuffleSplit\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=(val_ratio + test_ratio), random_state=random_state)\n",
    "\n",
    "    # Podział na zbiór treningowy i tymczasowy (walidacja + test)\n",
    "    for train_idx, temp_idx in sss.split(ids, classes):\n",
    "        train_ids = ids[train_idx]\n",
    "        temp_ids = ids[temp_idx]\n",
    "\n",
    "    # Podział tymczasowego zbioru na walidację i test\n",
    "    sss_temp = StratifiedShuffleSplit(n_splits=1, test_size=test_ratio / (val_ratio + test_ratio), random_state=random_state)\n",
    "    for val_idx, test_idx in sss_temp.split(temp_ids, classes[temp_idx]):\n",
    "        val_ids = temp_ids[val_idx]\n",
    "        test_ids = temp_ids[test_idx]\n",
    "\n",
    "    # Tworzenie zbiorów na podstawie podzielonych `id`\n",
    "    train_df = df[df['id'].isin(train_ids)]\n",
    "    val_df = df[df['id'].isin(val_ids)]\n",
    "    test_df = df[df['id'].isin(test_ids)]\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# Przykład użycia\n",
    "input_csv = \"CSV/dataset/dataset.csv\"  # Plik wejściowy CSV\n",
    "train_df, val_df, test_df = split_dataset(input_csv)\n",
    "\n",
    "# Zapisz zbiory do plików CSV\n",
    "train_df.to_csv(\"CSV/dataset/train.csv\", index=False)\n",
    "val_df.to_csv(\"CSV/dataset/val.csv\", index=False)\n",
    "test_df.to_csv(\"CSV/dataset/test.csv\", index=False)\n",
    "\n",
    "# Wyświetl liczność zbiorów\n",
    "print(f\"Liczba ziaren w zbiorze treningowym: {train_df['id'].nunique()} (obrazy: {len(train_df)})\")\n",
    "print(f\"Liczba ziaren w zbiorze walidacyjnym: {val_df['id'].nunique()} (obrazy: {len(val_df)})\")\n",
    "print(f\"Liczba ziaren w zbiorze testowym: {test_df['id'].nunique()} (obrazy: {len(test_df)})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnn_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
