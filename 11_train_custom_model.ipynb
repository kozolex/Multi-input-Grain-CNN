{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Katalog 'training_results_80_224_2' utworzony.\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1086/1086 [05:08<00:00,  3.52batch/s, loss=2.1554]\n",
      "Validation: 100%|██████████| 233/233 [00:49<00:00,  4.74batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 1\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1086/1086 [03:53<00:00,  4.66batch/s, loss=1.5260]\n",
      "Validation: 100%|██████████| 233/233 [00:42<00:00,  5.43batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 2\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1086/1086 [03:53<00:00,  4.65batch/s, loss=1.1673]\n",
      "Validation: 100%|██████████| 233/233 [00:43<00:00,  5.38batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 3\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1086/1086 [03:47<00:00,  4.77batch/s, loss=0.9130]\n",
      "Validation: 100%|██████████| 233/233 [00:41<00:00,  5.62batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 4\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1086/1086 [03:46<00:00,  4.79batch/s, loss=0.7284]\n",
      "Validation: 100%|██████████| 233/233 [00:41<00:00,  5.59batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 5\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1086/1086 [03:47<00:00,  4.78batch/s, loss=0.6075]\n",
      "Validation: 100%|██████████| 233/233 [00:41<00:00,  5.67batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 6\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1086/1086 [03:47<00:00,  4.77batch/s, loss=0.5225]\n",
      "Validation: 100%|██████████| 233/233 [00:41<00:00,  5.66batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 7\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1086/1086 [03:47<00:00,  4.77batch/s, loss=0.4692]\n",
      "Validation: 100%|██████████| 233/233 [00:41<00:00,  5.63batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 1 epoch(s)\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1086/1086 [03:47<00:00,  4.78batch/s, loss=0.4218]\n",
      "Validation: 100%|██████████| 233/233 [00:41<00:00,  5.61batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 9\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1086/1086 [03:47<00:00,  4.78batch/s, loss=0.3844]\n",
      "Validation: 100%|██████████| 233/233 [00:41<00:00,  5.60batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 10\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1086/1086 [03:47<00:00,  4.76batch/s, loss=0.3525]\n",
      "Validation: 100%|██████████| 233/233 [00:41<00:00,  5.63batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 11\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1086/1086 [03:47<00:00,  4.78batch/s, loss=0.3251]\n",
      "Validation: 100%|██████████| 233/233 [00:41<00:00,  5.60batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 1 epoch(s)\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1086/1086 [03:47<00:00,  4.77batch/s, loss=0.3038]\n",
      "Validation: 100%|██████████| 233/233 [00:41<00:00,  5.61batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 13\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1086/1086 [03:47<00:00,  4.78batch/s, loss=0.2820]\n",
      "Validation: 100%|██████████| 233/233 [00:41<00:00,  5.60batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 1 epoch(s)\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1086/1086 [03:47<00:00,  4.78batch/s, loss=0.2615]\n",
      "Validation: 100%|██████████| 233/233 [00:41<00:00,  5.59batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 2 epoch(s)\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1086/1086 [03:46<00:00,  4.79batch/s, loss=0.2489]\n",
      "Validation: 100%|██████████| 233/233 [00:40<00:00,  5.72batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 16\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1086/1086 [03:46<00:00,  4.79batch/s, loss=0.2316]\n",
      "Validation: 100%|██████████| 233/233 [00:41<00:00,  5.59batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 17\n",
      "Epoch 18/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1086/1086 [03:47<00:00,  4.78batch/s, loss=0.2219]\n",
      "Validation: 100%|██████████| 233/233 [00:41<00:00,  5.61batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 1 epoch(s)\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1086/1086 [03:47<00:00,  4.78batch/s, loss=0.2152]\n",
      "Validation: 100%|██████████| 233/233 [00:41<00:00,  5.58batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 19\n",
      "Epoch 20/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1086/1086 [03:47<00:00,  4.78batch/s, loss=0.2017]\n",
      "Validation: 100%|██████████| 233/233 [00:41<00:00,  5.62batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 1 epoch(s)\n",
      "Epoch 21/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1086/1086 [03:47<00:00,  4.78batch/s, loss=0.1931]\n",
      "Validation: 100%|██████████| 233/233 [00:41<00:00,  5.60batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 2 epoch(s)\n",
      "Epoch 22/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1086/1086 [03:47<00:00,  4.78batch/s, loss=0.1823]\n",
      "Validation: 100%|██████████| 233/233 [00:41<00:00,  5.61batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 22\n",
      "Epoch 23/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1086/1086 [03:47<00:00,  4.78batch/s, loss=0.1776]\n",
      "Validation: 100%|██████████| 233/233 [00:41<00:00,  5.61batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 1 epoch(s)\n",
      "Epoch 24/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1086/1086 [03:46<00:00,  4.79batch/s, loss=0.1686]\n",
      "Validation: 100%|██████████| 233/233 [00:41<00:00,  5.63batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 2 epoch(s)\n",
      "Epoch 25/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1086/1086 [03:47<00:00,  4.78batch/s, loss=0.1644]\n",
      "Validation: 100%|██████████| 233/233 [00:41<00:00,  5.60batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 3 epoch(s)\n",
      "Epoch 26/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1086/1086 [03:47<00:00,  4.77batch/s, loss=0.1523]\n",
      "Validation: 100%|██████████| 233/233 [00:41<00:00,  5.59batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 4 epoch(s)\n",
      "Epoch 27/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1086/1086 [03:47<00:00,  4.77batch/s, loss=0.1494]\n",
      "Validation: 100%|██████████| 233/233 [00:41<00:00,  5.67batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 27\n",
      "Epoch 28/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1086/1086 [03:47<00:00,  4.78batch/s, loss=0.1392]\n",
      "Validation: 100%|██████████| 233/233 [00:41<00:00,  5.56batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 1 epoch(s)\n",
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1086/1086 [03:46<00:00,  4.79batch/s, loss=0.1385]\n",
      "Validation: 100%|██████████| 233/233 [00:41<00:00,  5.68batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 29\n",
      "Epoch 30/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1086/1086 [03:46<00:00,  4.80batch/s, loss=0.1318]\n",
      "Validation: 100%|██████████| 233/233 [00:41<00:00,  5.63batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 30\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1086/1086 [03:45<00:00,  4.81batch/s, loss=0.1243]\n",
      "Validation: 100%|██████████| 233/233 [00:41<00:00,  5.62batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 1 epoch(s)\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1086/1086 [03:46<00:00,  4.80batch/s, loss=0.1249]\n",
      "Validation: 100%|██████████| 233/233 [00:41<00:00,  5.65batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 32\n",
      "Epoch 33/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1086/1086 [03:46<00:00,  4.80batch/s, loss=0.1228]\n",
      "Validation: 100%|██████████| 233/233 [00:41<00:00,  5.64batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 1 epoch(s)\n",
      "Epoch 34/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1086/1086 [03:46<00:00,  4.80batch/s, loss=0.1147]\n",
      "Validation: 100%|██████████| 233/233 [00:41<00:00,  5.64batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 2 epoch(s)\n",
      "Epoch 35/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1086/1086 [03:46<00:00,  4.80batch/s, loss=0.1080]\n",
      "Validation: 100%|██████████| 233/233 [00:41<00:00,  5.59batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 3 epoch(s)\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1086/1086 [03:48<00:00,  4.76batch/s, loss=0.1106]\n",
      "Validation: 100%|██████████| 233/233 [00:41<00:00,  5.64batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 4 epoch(s)\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1086/1086 [03:46<00:00,  4.79batch/s, loss=0.1036]\n",
      "Validation: 100%|██████████| 233/233 [00:41<00:00,  5.56batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 5 epoch(s)\n",
      "Early stopping triggered. Training stopped.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import multiModel as mm\n",
    "from multiModel import MultiInputModel\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "#Zapisz wyniki w \n",
    "path_results = \"training_results_80_224_2\"\n",
    "# Utwórz katalog docelowy, jeśli nie istnieje\n",
    "if not os.path.exists(path_results):\n",
    "    print(f\"Katalog '{path_results}' utworzony.\")\n",
    "    # Tworzenie katalogu, jeśli nie istnieje\n",
    "    os.makedirs(path_results, exist_ok=True)\n",
    "\n",
    "# Włącz blokowanie błędów CUDA\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# Załaduj dane\n",
    "train_dataset = mm.MultiInputDataset(\"CSV/dataset/train_80.csv\", transform_rgb=mm.transform_rgb_224, transform_binary=mm.transform_binary_224)\n",
    "val_dataset = mm.MultiInputDataset(\"CSV/dataset/val_80.csv\", transform_rgb=mm.transform_rgb_224, transform_binary=mm.transform_binary_224)\n",
    "test_dataset = mm.MultiInputDataset(\"CSV/dataset/test_80.csv\", transform_rgb=mm.transform_rgb_224, transform_binary=mm.transform_binary_224)\n",
    "\n",
    "\n",
    "# Inicjalizacja modelu\n",
    "model = mm.CustomMultiInputModel(num_classes=11)\n",
    "model = model.to(\"cuda\")  # Jeśli masz GPU\n",
    "\n",
    "#Dynamicznie przydzielany batch_size\n",
    "\n",
    "batch_size = 64\n",
    "model_name = \"mm\" + str(batch_size)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Optymalizator i funkcja straty\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Zapis logów\n",
    "log_file = f\"{path_results}/training_log_{model_name}.txt\"\n",
    "with open(log_file, \"w\") as f:\n",
    "    f.write(\"epoch,train_loss,val_loss,train_accuracy,val_accuracy,train_precision,val_precision,train_recall,val_recall,train_f1,val_f1\\n\")\n",
    "\n",
    "# Wczesne zatrzymanie - parametry\n",
    "early_stop_patience = 5  # Liczba epok bez poprawy\n",
    "best_val_loss = float(\"inf\")\n",
    "patience_counter = 0\n",
    "best_model_path = f\"{path_results}/best_model_{model_name}.pth\"\n",
    "\n",
    "# Pętla treningowa\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    # === TRENING ===\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_true = []\n",
    "    train_pred = []\n",
    "\n",
    "    # Dodaj pasek postępu do pętli batchy\n",
    "    with tqdm(total=len(train_loader), desc=\"Training\", unit=\"batch\") as pbar:\n",
    "        for t_image, b_image, s_image, labels in train_loader:\n",
    "            t_image, b_image, s_image, labels = (\n",
    "                t_image.to(\"cuda\"),\n",
    "                b_image.to(\"cuda\"),\n",
    "                s_image.to(\"cuda\"),\n",
    "                labels.to(\"cuda\")\n",
    "            )\n",
    "\n",
    "            # Oblicz predykcje i stratę\n",
    "            outputs = model(t_image, b_image, s_image)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_true.extend(labels.cpu().numpy())\n",
    "            train_pred.extend(predicted.cpu().numpy())\n",
    "            pbar.set_postfix({\"loss\": f\"{train_loss / (pbar.n + 1):.4f}\"})  # Wyświetl średnią stratę\n",
    "            pbar.update(1)  # Aktualizuj pasek postępu o 1 krok\n",
    "\n",
    "    train_loss /= len(train_loader)  # Średnia strata w treningu\n",
    "    train_accuracy = accuracy_score(train_true, train_pred)\n",
    "    train_precision = precision_score(train_true, train_pred, average=\"weighted\")\n",
    "    train_recall = recall_score(train_true, train_pred, average=\"weighted\")\n",
    "    train_f1 = f1_score(train_true, train_pred, average=\"weighted\")\n",
    "    #print(f\"Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "    # === WALIDACJA ===\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_true = []\n",
    "    val_pred = []\n",
    "\n",
    "    with torch.no_grad():  # Wyłącz gradienty\n",
    "        with tqdm(total=len(val_loader), desc=\"Validation\", unit=\"batch\") as pbar_val:\n",
    "            for t_image, b_image, s_image, labels in val_loader:\n",
    "                t_image, b_image, s_image, labels = (\n",
    "                    t_image.to(\"cuda\"),\n",
    "                    b_image.to(\"cuda\"),\n",
    "                    s_image.to(\"cuda\"),\n",
    "                    labels.to(\"cuda\")\n",
    "                )\n",
    "                outputs = model(t_image, b_image, s_image)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_true.extend(labels.cpu().numpy())\n",
    "                val_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "                pbar_val.update(1)  # Aktualizuj pasek postępu walidacji\n",
    "\n",
    "    val_loss /= len(val_loader)  # Średnia strata w walidacji\n",
    "    val_accuracy = accuracy_score(val_true, val_pred)\n",
    "    val_precision = precision_score(val_true, val_pred, average=\"weighted\")\n",
    "    val_recall = recall_score(val_true, val_pred, average=\"weighted\")\n",
    "    val_f1 = f1_score(val_true, val_pred, average=\"weighted\")\n",
    "    #print(f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # === LOGI ===\n",
    "    #print(f\"Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(\n",
    "            f\"{epoch + 1},{train_loss:.4f},{val_loss:.4f},{train_accuracy:.4f},{val_accuracy:.4f},\"\n",
    "            f\"{train_precision:.4f},{val_precision:.4f},{train_recall:.4f},{val_recall:.4f},{train_f1:.4f},{val_f1:.4f}\\n\"\n",
    "        )\n",
    "\n",
    "    # === WCZESNE ZATRZYMANIE ===\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # Zapis najlepszego modelu\n",
    "        torch.save(model, best_model_path) #Zapisanie modelu i architektury w pliku pth\n",
    "        print(f\"Best model saved at epoch {epoch + 1}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement in val loss for {patience_counter} epoch(s)\")\n",
    "\n",
    "    if patience_counter >= early_stop_patience:\n",
    "        print(\"Early stopping triggered. Training stopped.\")\n",
    "        break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55378/4230748760.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(best_model_path)\n",
      "Testing: 100%|██████████| 233/233 [01:06<00:00,  3.52batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results saved to training_results_80_224_2/test_results_mm64.txt\n",
      "Confusion matrix saved to training_results_80_224_2/confusion_matrix_mm64.png\n",
      "Test Loss: 0.2039, Accuracy: 0.9367, Precision: 0.9383, Recall: 0.9367, F1: 0.9366\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from multiModel import MultiInputModel, MultiInputDataset\n",
    "import multiModel as mm\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set CUDA error blocking\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# Paths and parameters\n",
    "path_results = \"training_results_80_224_2\"\n",
    "model_name = \"mm64\"\n",
    "best_model_path = f\"{path_results}/best_model_{model_name}.pth\"\n",
    "test_csv = \"CSV/dataset/test_80.csv\"\n",
    "batch_size = 64\n",
    "num_classes = 11\n",
    "class_names = [str(i) for i in range(num_classes)]  # Update with actual class names if available\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "os.makedirs(path_results, exist_ok=True)\n",
    "\n",
    "# Load test dataset\n",
    "test_dataset = MultiInputDataset(test_csv, transform_rgb=mm.transform_rgb_224, transform_binary=mm.transform_binary_224)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Load the trained model\n",
    "model = torch.load(best_model_path)\n",
    "model = model.to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Test loop\n",
    "test_loss = 0\n",
    "test_true = []\n",
    "test_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    with tqdm(total=len(test_loader), desc=\"Testing\", unit=\"batch\") as pbar:\n",
    "        for t_image, b_image, s_image, labels in test_loader:\n",
    "            t_image, b_image, s_image, labels = (\n",
    "                t_image.to(\"cuda\"),\n",
    "                b_image.to(\"cuda\"),\n",
    "                s_image.to(\"cuda\"),\n",
    "                labels.to(\"cuda\")\n",
    "            )\n",
    "            outputs = model(t_image, b_image, s_image)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            test_true.extend(labels.cpu().numpy())\n",
    "            test_pred.extend(predicted.cpu().numpy())\n",
    "            pbar.update(1)\n",
    "\n",
    "# Calculate metrics\n",
    "test_loss /= len(test_loader)\n",
    "test_accuracy = accuracy_score(test_true, test_pred)\n",
    "test_precision = precision_score(test_true, test_pred, average=\"weighted\")\n",
    "test_recall = recall_score(test_true, test_pred, average=\"weighted\")\n",
    "test_f1 = f1_score(test_true, test_pred, average=\"weighted\")\n",
    "\n",
    "# Log test metrics\n",
    "test_log_file = f\"{path_results}/test_results_{model_name}.txt\"\n",
    "with open(test_log_file, \"w\") as f:\n",
    "    f.write(\"test_loss,test_accuracy,test_precision,test_recall,test_f1\\n\")\n",
    "    f.write(f\"{test_loss:.4f},{test_accuracy:.4f},{test_precision:.4f},{test_recall:.4f},{test_f1:.4f}\\n\")\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_true, test_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save confusion matrix plot\n",
    "cm_plot_path = f\"{path_results}/confusion_matrix_{model_name}.png\"\n",
    "plt.savefig(cm_plot_path)\n",
    "plt.close()\n",
    "\n",
    "print(f\"Test results saved to {test_log_file}\")\n",
    "print(f\"Confusion matrix saved to {cm_plot_path}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}, Precision: {test_precision:.4f}, \"\n",
    "      f\"Recall: {test_recall:.4f}, F1: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_48901/4079286112.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(best_model_path)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'criterion' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 39\u001b[0m\n\u001b[1;32m     32\u001b[0m t_image, b_image, s_image, labels \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     33\u001b[0m     t_image\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     34\u001b[0m     b_image\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     35\u001b[0m     s_image\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     36\u001b[0m     labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     38\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(t_image, b_image, s_image)\n\u001b[0;32m---> 39\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m(outputs, labels)\n\u001b[1;32m     40\u001b[0m test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Oblicz dokładność\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'criterion' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import multiModel as mm\n",
    "from multiModel import MultiInputModel\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "path_results = \"training_results_80_224_2\"\n",
    "test_dataset = mm.MultiInputDataset(\"CSV/dataset/test_80.csv\", transform_rgb=mm.transform_rgb_224, transform_binary=mm.transform_binary_224)\n",
    "batch_size = 64\n",
    "model_name = \"mm\" + str(batch_size)\n",
    "best_model_path = f\"{path_results}/best_model_{model_name}.pth\"\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "# === TEST ===\n",
    "\n",
    "# Inicjalizacja modelu\n",
    "#model = mm.CustomMultiInputModel(num_classes=11)\n",
    "model = torch.load(best_model_path)\n",
    "#model = model.to(\"cuda\")  # Jeśli masz GPU\n",
    "# Wczytaj najlepszy model\n",
    "#model= torch.load(best_model_path)\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for t_image, b_image, s_image, labels in test_loader:\n",
    "        t_image, b_image, s_image, labels = (\n",
    "            t_image.to(\"cuda\"),\n",
    "            b_image.to(\"cuda\"),\n",
    "            s_image.to(\"cuda\"),\n",
    "            labels.to(\"cuda\")\n",
    "        )\n",
    "        outputs = model(t_image, b_image, s_image)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Oblicz dokładność\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "accuracy = correct / total\n",
    "\n",
    "# Zapis wyniku testu\n",
    "log_file_test = f\"training_results/test_log_{model_name}.txt\"\n",
    "with open(log_file_test, \"w\") as f_t:\n",
    "        f_t.write(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Zwalnianie pamięci po zakończeniu pracy z modelem\n",
    "del model  # Usuń model z pamięci\n",
    "torch.cuda.empty_cache()  # Wyczyść pamięć GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnn_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
