{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17b3fe4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing efficientnet_b0...\n",
      "Best epoch for efficientnet_b0: 14.0, Val Loss: 0.0687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_99485/491750611.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(best_model_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing efficientnet_v2_m...\n",
      "Best epoch for efficientnet_v2_m: 20.0, Val Loss: 0.0980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_99485/491750611.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(best_model_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing mobilenet_v3_small...\n",
      "Best epoch for mobilenet_v3_small: 8.0, Val Loss: 0.1225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_99485/491750611.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(best_model_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing resnet34...\n",
      "Best epoch for resnet34: 22.0, Val Loss: 0.1361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_99485/491750611.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(best_model_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparative Report:\n",
      "             Model  Test Loss  Test Accuracy  Test Precision  Test Recall  Test F1  Inference Time (s)  Inference Time per Sample (ms)  GPU Memory (MB)  Parameters\n",
      "   efficientnet_b0   0.069377       0.981860        0.981935     0.981860 0.981808           76.491440                        5.139172        29.054199     5393383\n",
      " efficientnet_v2_m   0.096431       0.976619        0.976762     0.976619 0.976598          125.810054                        8.452705       221.755859    54244191\n",
      "mobilenet_v3_small   0.109049       0.968288        0.968789     0.968288 0.968320           65.626962                        4.409229        14.304199     1591947\n",
      "          resnet34   0.130095       0.967415        0.967532     0.967415 0.967436           79.708193                        5.355294        92.095703    21884075\n",
      "\n",
      "Report saved to training_results_224_2/comparative_report_batch64.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import multiModel as mm\n",
    "from multiModel import MultiInputModel\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Ścieżki i parametry\n",
    "path_results = \"training_results_224_2\"\n",
    "models_list = ['efficientnet_b0', 'efficientnet_v2_m', 'mobilenet_v3_small', 'resnet34']#, 'swin_t', 'vit_b_16']\n",
    "batch_sizes = {\n",
    "    'efficientnet_b0': 64,      #32,\n",
    "    'mobilenet_v3_small': 64,   #32,\n",
    "    'efficientnet_v2_m': 64,    #,16,\n",
    "    'resnet34': 64,             #32,\n",
    "    'swin_t': 32,\n",
    "    'vit_b_16': 16\n",
    "}\n",
    "num_classes = 11\n",
    "class_names = [str(i) for i in range(num_classes)]  # Zaktualizuj z rzeczywistymi nazwami klas, jeśli dostępne\n",
    "\n",
    "# Wczytanie danych testowych\n",
    "test_dataset = mm.MultiInputDataset(\"CSV/dataset/test_224.csv\", transform_rgb=mm.transform_rgb_224, transform_binary=mm.transform_binary_224)\n",
    "\n",
    "# Funkcja do wczytywania logów\n",
    "def load_logs(log_file):\n",
    "    df = pd.read_csv(log_file)\n",
    "    return df\n",
    "\n",
    "# Funkcja do obliczania metryk\n",
    "def calculate_metrics(true_labels, pred_labels):\n",
    "    accuracy = accuracy_score(true_labels, pred_labels)\n",
    "    precision = precision_score(true_labels, pred_labels, average=\"weighted\")\n",
    "    recall = recall_score(true_labels, pred_labels, average=\"weighted\")\n",
    "    f1 = f1_score(true_labels, pred_labels, average=\"weighted\")\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Funkcja do testowania modelu\n",
    "def test_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_true = []\n",
    "    test_pred = []\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for t_image, b_image, s_image, labels in test_loader:\n",
    "            t_image, b_image, s_image, labels = (\n",
    "                t_image.to(device),\n",
    "                b_image.to(device),\n",
    "                s_image.to(device),\n",
    "                labels.to(device)\n",
    "            )\n",
    "            outputs = model(t_image, b_image, s_image)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            test_true.extend(labels.cpu().numpy())\n",
    "            test_pred.extend(predicted.cpu().numpy())\n",
    "    end_time = time.time()\n",
    "    test_loss /= len(test_loader)\n",
    "    inference_time = end_time - start_time\n",
    "    return test_loss, test_true, test_pred, inference_time\n",
    "\n",
    "# Funkcja do mierzenia zużycia pamięci GPU\n",
    "def get_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.memory_allocated() / 1024**2  # MB\n",
    "    return 0\n",
    "\n",
    "# Funkcja do liczenia parametrów modelu\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Inicjalizacja raportu\n",
    "report = {\n",
    "    \"Model\": [],\n",
    "    \"Test Loss\": [],\n",
    "    \"Test Accuracy\": [],\n",
    "    \"Test Precision\": [],\n",
    "    \"Test Recall\": [],\n",
    "    \"Test F1\": [],\n",
    "    \"Inference Time (s)\": [],\n",
    "    \"Inference Time per Sample (ms)\": [],\n",
    "    \"GPU Memory (MB)\": [],\n",
    "    \"Parameters\": []\n",
    "}\n",
    "\n",
    "# Pętla po modelach\n",
    "for model_name in models_list:\n",
    "    print(f\"Processing {model_name}...\")\n",
    "    # Wczytanie logów\n",
    "    log_file = f\"{path_results}/training_log_{model_name}.txt\"\n",
    "    logs = load_logs(log_file)\n",
    "    # Znalezienie najlepszej epoki na podstawie val_loss\n",
    "    best_epoch = logs.loc[logs['val_loss'].idxmin()]\n",
    "    print(f\"Best epoch for {model_name}: {best_epoch['epoch']}, Val Loss: {best_epoch['val_loss']:.4f}\")\n",
    "\n",
    "    # Wczytanie najlepszego modelu\n",
    "    best_model_path = f\"{path_results}/best_model_{model_name}.pth\"\n",
    "    model = torch.load(best_model_path)\n",
    "    model = model.to(\"cuda\")\n",
    "    model.eval()\n",
    "\n",
    "    # Ustawienie batch_size\n",
    "    batch_size = batch_sizes[model_name]\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Testowanie modelu\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    test_loss, test_true, test_pred, inference_time = test_model(model, test_loader, criterion, \"cuda\")\n",
    "\n",
    "    # Obliczenie metryk\n",
    "    accuracy, precision, recall, f1 = calculate_metrics(test_true, test_pred)\n",
    "\n",
    "    # Obliczenie czasu inferencji na próbkę\n",
    "    num_samples = len(test_dataset)\n",
    "    inference_time_per_sample = (inference_time / num_samples) * 1000  # ms\n",
    "\n",
    "    # Pomiar zużycia pamięci GPU\n",
    "    gpu_memory = get_gpu_memory()\n",
    "\n",
    "    # Liczenie parametrów\n",
    "    num_params = count_parameters(model)\n",
    "\n",
    "    # Dodanie do raportu\n",
    "    report[\"Model\"].append(model_name)\n",
    "    report[\"Test Loss\"].append(test_loss)\n",
    "    report[\"Test Accuracy\"].append(accuracy)\n",
    "    report[\"Test Precision\"].append(precision)\n",
    "    report[\"Test Recall\"].append(recall)\n",
    "    report[\"Test F1\"].append(f1)\n",
    "    report[\"Inference Time (s)\"].append(inference_time)\n",
    "    report[\"Inference Time per Sample (ms)\"].append(inference_time_per_sample)\n",
    "    report[\"GPU Memory (MB)\"].append(gpu_memory)\n",
    "    report[\"Parameters\"].append(num_params)\n",
    "\n",
    "    # Zwalnianie pamięci\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Tworzenie DataFrame z raportem\n",
    "report_df = pd.DataFrame(report)\n",
    "\n",
    "# Sortowanie raportu po Test Accuracy\n",
    "report_df = report_df.sort_values(by=\"Test Accuracy\", ascending=False)\n",
    "\n",
    "# Wyświetlenie raportu\n",
    "print(\"\\nComparative Report:\")\n",
    "print(report_df.to_string(index=False))\n",
    "\n",
    "# Zapisanie raportu do pliku CSV\n",
    "report_csv_path = f\"{path_results}/comparative_report_batch64.csv\"\n",
    "report_df.to_csv(report_csv_path, index=False)\n",
    "print(f\"\\nReport saved to {report_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60c465bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing mm64...\n",
      "Best epoch for mm64: 32.0, Val Loss: 0.2036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_99485/1488903860.py:108: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(best_model_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparative Report:\n",
      "Model  Test Loss  Test Accuracy  Test Precision  Test Recall  Test F1  Inference Time (s)  Inference Time per Sample (ms)  GPU Memory (MB)  Parameters\n",
      " mm64   0.203917       0.936711        0.938295     0.936711 0.936575           37.242671                        2.502195         9.644043      397355\n",
      "\n",
      "Report saved to training_results_80_224_2/comparative_report_batch64.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import multiModel as mm\n",
    "from multiModel import MultiInputModel\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Ścieżki i parametry\n",
    "path_results = \"training_results_80_224_2\"\n",
    "models_list = ['mm64']\n",
    "batch_sizes = {\n",
    "    'efficientnet_b0': 64,      #32,\n",
    "    'mobilenet_v3_small': 64,   #32,\n",
    "    'efficientnet_v2_m': 64,    #,16,\n",
    "    'resnet34': 64,             #32,\n",
    "    'swin_t': 32,\n",
    "    'vit_b_16': 16,\n",
    "    'mm64': 64\n",
    "}\n",
    "num_classes = 11\n",
    "class_names = [str(i) for i in range(num_classes)]  # Zaktualizuj z rzeczywistymi nazwami klas, jeśli dostępne\n",
    "\n",
    "# Wczytanie danych testowych\n",
    "test_dataset = mm.MultiInputDataset(\"CSV/dataset/test_80.csv\", transform_rgb=mm.transform_rgb_224, transform_binary=mm.transform_binary_224)\n",
    "\n",
    "# Funkcja do wczytywania logów\n",
    "def load_logs(log_file):\n",
    "    df = pd.read_csv(log_file)\n",
    "    return df\n",
    "\n",
    "# Funkcja do obliczania metryk\n",
    "def calculate_metrics(true_labels, pred_labels):\n",
    "    accuracy = accuracy_score(true_labels, pred_labels)\n",
    "    precision = precision_score(true_labels, pred_labels, average=\"weighted\")\n",
    "    recall = recall_score(true_labels, pred_labels, average=\"weighted\")\n",
    "    f1 = f1_score(true_labels, pred_labels, average=\"weighted\")\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Funkcja do testowania modelu\n",
    "def test_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_true = []\n",
    "    test_pred = []\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for t_image, b_image, s_image, labels in test_loader:\n",
    "            t_image, b_image, s_image, labels = (\n",
    "                t_image.to(device),\n",
    "                b_image.to(device),\n",
    "                s_image.to(device),\n",
    "                labels.to(device)\n",
    "            )\n",
    "            outputs = model(t_image, b_image, s_image)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            test_true.extend(labels.cpu().numpy())\n",
    "            test_pred.extend(predicted.cpu().numpy())\n",
    "    end_time = time.time()\n",
    "    test_loss /= len(test_loader)\n",
    "    inference_time = end_time - start_time\n",
    "    return test_loss, test_true, test_pred, inference_time\n",
    "\n",
    "# Funkcja do mierzenia zużycia pamięci GPU\n",
    "def get_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.memory_allocated() / 1024**2  # MB\n",
    "    return 0\n",
    "\n",
    "# Funkcja do liczenia parametrów modelu\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Inicjalizacja raportu\n",
    "report = {\n",
    "    \"Model\": [],\n",
    "    \"Test Loss\": [],\n",
    "    \"Test Accuracy\": [],\n",
    "    \"Test Precision\": [],\n",
    "    \"Test Recall\": [],\n",
    "    \"Test F1\": [],\n",
    "    \"Inference Time (s)\": [],\n",
    "    \"Inference Time per Sample (ms)\": [],\n",
    "    \"GPU Memory (MB)\": [],\n",
    "    \"Parameters\": []\n",
    "}\n",
    "\n",
    "# Pętla po modelach\n",
    "for model_name in models_list:\n",
    "    print(f\"Processing {model_name}...\")\n",
    "    # Wczytanie logów\n",
    "    log_file = f\"{path_results}/training_log_{model_name}.txt\"\n",
    "    logs = load_logs(log_file)\n",
    "    # Znalezienie najlepszej epoki na podstawie val_loss\n",
    "    best_epoch = logs.loc[logs['val_loss'].idxmin()]\n",
    "    print(f\"Best epoch for {model_name}: {best_epoch['epoch']}, Val Loss: {best_epoch['val_loss']:.4f}\")\n",
    "\n",
    "    # Wczytanie najlepszego modelu\n",
    "    best_model_path = f\"{path_results}/best_model_{model_name}.pth\"\n",
    "    model = torch.load(best_model_path)\n",
    "    model = model.to(\"cuda\")\n",
    "    model.eval()\n",
    "\n",
    "    # Ustawienie batch_size\n",
    "    batch_size = batch_sizes[model_name]\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Testowanie modelu\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    test_loss, test_true, test_pred, inference_time = test_model(model, test_loader, criterion, \"cuda\")\n",
    "\n",
    "    # Obliczenie metryk\n",
    "    accuracy, precision, recall, f1 = calculate_metrics(test_true, test_pred)\n",
    "\n",
    "    # Obliczenie czasu inferencji na próbkę\n",
    "    num_samples = len(test_dataset)\n",
    "    inference_time_per_sample = (inference_time / num_samples) * 1000  # ms\n",
    "\n",
    "    # Pomiar zużycia pamięci GPU\n",
    "    gpu_memory = get_gpu_memory()\n",
    "\n",
    "    # Liczenie parametrów\n",
    "    num_params = count_parameters(model)\n",
    "\n",
    "    # Dodanie do raportu\n",
    "    report[\"Model\"].append(model_name)\n",
    "    report[\"Test Loss\"].append(test_loss)\n",
    "    report[\"Test Accuracy\"].append(accuracy)\n",
    "    report[\"Test Precision\"].append(precision)\n",
    "    report[\"Test Recall\"].append(recall)\n",
    "    report[\"Test F1\"].append(f1)\n",
    "    report[\"Inference Time (s)\"].append(inference_time)\n",
    "    report[\"Inference Time per Sample (ms)\"].append(inference_time_per_sample)\n",
    "    report[\"GPU Memory (MB)\"].append(gpu_memory)\n",
    "    report[\"Parameters\"].append(num_params)\n",
    "\n",
    "    # Zwalnianie pamięci\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Tworzenie DataFrame z raportem\n",
    "report_df = pd.DataFrame(report)\n",
    "\n",
    "# Sortowanie raportu po Test Accuracy\n",
    "report_df = report_df.sort_values(by=\"Test Accuracy\", ascending=False)\n",
    "\n",
    "# Wyświetlenie raportu\n",
    "print(\"\\nComparative Report:\")\n",
    "print(report_df.to_string(index=False))\n",
    "\n",
    "# Zapisanie raportu do pliku CSV\n",
    "report_csv_path = f\"{path_results}/comparative_report_batch64.csv\"\n",
    "report_df.to_csv(report_csv_path, index=False)\n",
    "print(f\"\\nReport saved to {report_csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnn_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
