{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modele takie jak maxvit_t, swin_t czy vit_b_16 są dostępne dopiero od nowszych wersji torchvision (od wersji 0.13 lub 0.14). Jeśli użyjesz wcześniejszych wersji, pojawią się błędy, np. AttributeError: module torchvision.models has no attribute maxvit_t.\n",
    "Sugestia: Sprawdź wersję biblioteki torchvision w swoim środowisku:\n",
    "import torchvision\n",
    "print(torchvision.__version__)\n",
    "Jeśli używasz starszej wersji, zaktualizuj ją:\n",
    "pip install --upgrade torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "\n",
    "class MultiInputModel(nn.Module):\n",
    "    def __init__(self, num_classes=11, base_model='efficientnet_v2_m', filter_num_base=4):\n",
    "        super(MultiInputModel, self).__init__()\n",
    "        \n",
    "        # Inicjalizacja modelu RGB\n",
    "        self.base_model = base_model\n",
    "        self.rgb_model, self.base_model_output_size = self._initialize_rgb_model(base_model)\n",
    "        print(f\"Model: {base_model}, base_model_output_size: {self.base_model_output_size}\")\n",
    "\n",
    "        # Inicjalizacja modelu binarnego\n",
    "        self.binary_model = nn.Sequential(\n",
    "            nn.Conv2d(1, filter_num_base * 2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(filter_num_base * 2, filter_num_base * 4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(filter_num_base * 4, 128),  # Dopasowanie wyjścia do 128\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.binary_model_output_size = 128\n",
    "\n",
    "        # Warstwa łącząca\n",
    "        total_input_size = self.base_model_output_size * 2 + self.binary_model_output_size\n",
    "        print(f\"Total input size to fc: {total_input_size}\")\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(total_input_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def _initialize_rgb_model(self, base_model):\n",
    "        \"\"\"\n",
    "        Inicjalizuje wybrany model sieci RGB i zwraca model oraz rozmiar jego wyjścia.\n",
    "        \"\"\"\n",
    "        if base_model.startswith('efficientnet'):  # Obsługa EfficientNet i EfficientNetV2\n",
    "            model = getattr(models, base_model)(pretrained=True)\n",
    "            model.classifier = nn.Identity()\n",
    "            if base_model.startswith('efficientnet_v2'):\n",
    "                return model, 1280  # Wyjście dla EfficientNetV2-M\n",
    "            return model, 1280  # Wyjście dla EfficientNet-B0/B1\n",
    "        \n",
    "        elif base_model == 'googlenet':\n",
    "            model = models.googlenet(pretrained=True)\n",
    "            model.fc = nn.Identity()\n",
    "            return model, 1024\n",
    "        \n",
    "        elif base_model == 'inception_v3':\n",
    "            model = models.inception_v3(pretrained=True, aux_logits=False)  # Wyłącz dodatkowe głowice\n",
    "            model.fc = nn.Identity()\n",
    "            return model, 2048\n",
    "        \n",
    "        elif base_model == 'mobilenet_v2':\n",
    "            model = models.mobilenet_v2(pretrained=True)\n",
    "            model.classifier = nn.Identity()\n",
    "            return model, 1280\n",
    "        \n",
    "        elif base_model == 'mobilenet_v3_large' or base_model == 'mobilenet_v3_small':\n",
    "            model = getattr(models, base_model)(pretrained=True)\n",
    "            model.classifier = nn.Identity()\n",
    "            return model, 576\n",
    "        \n",
    "        elif base_model.startswith('resnet'):  # Obsługa ResNet (np. resnet18, resnet50)\n",
    "            model = getattr(models, base_model)(pretrained=True)\n",
    "            model.fc = nn.Identity()\n",
    "            return model, 2048 if '50' in base_model or '101' in base_model else 512  # Rozmiar zależny od wariantu\n",
    "        \n",
    "        elif base_model == 'swin_t':\n",
    "            model = models.swin_t(pretrained=True)\n",
    "            model.head = nn.Identity()\n",
    "            return model, 768\n",
    "        \n",
    "        elif base_model == 'vit_b_16':  # VisionTransformer\n",
    "            model = models.vit_b_16(pretrained=True)\n",
    "            model.heads = nn.Identity()\n",
    "            return model, 768\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported base model: {base_model}\")\n",
    "\n",
    "    def forward(self, t_image, b_image, s_image):\n",
    "        # Ekstrakcja cech dla widoków RGB\n",
    "        t_features = self.rgb_model(t_image)  # Widok T\n",
    "        b_features = self.rgb_model(b_image)  # Widok B\n",
    "        #print(f\"T Features: {t_features.shape}, B Features: {b_features.shape}\")\n",
    "\n",
    "        # Ekstrakcja cech dla obrazu binarnego\n",
    "        s_features = self.binary_model(s_image)\n",
    "        #print(f\"S Features: {s_features.shape}\")\n",
    "\n",
    "        # Połączenie cech\n",
    "        combined_features = torch.cat([t_features, b_features, s_features], dim=1)\n",
    "        #print(f\"Combined Features: {combined_features.shape}\")\n",
    "\n",
    "        # Klasyfikacja\n",
    "        output = self.fc(combined_features)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def get_input_size(base_model):\n",
    "        \"\"\"\n",
    "        Zwraca wymagane wymiary wejściowe dla danego modelu.\n",
    "        \n",
    "        Args:\n",
    "            base_model (str): Nazwa modelu bazowego.\n",
    "            \n",
    "        Returns:\n",
    "            tuple: Wymiary wejściowe modelu (wysokość, szerokość).\n",
    "        \"\"\"\n",
    "        if base_model.startswith('efficientnet') or base_model.startswith('mobilenet'):\n",
    "            return (224, 224)  # EfficientNet, MobileNet wymagają 224x224\n",
    "            \n",
    "        elif base_model == 'googlenet':\n",
    "            return (224, 224)  # GoogLeNet wymaga 224x224\n",
    "        \n",
    "        elif base_model == 'inception_v3':\n",
    "            return (299, 299)  # Inception V3 wymaga 299x299\n",
    "        \n",
    "        elif base_model == 'maxvit_t':\n",
    "            return (224, 224)  # MaxVit wymaga 224x224\n",
    "        \n",
    "        elif base_model.startswith('resnet'):\n",
    "            return (224, 224)  # ResNet (np. ResNet50/ResNet101) wymaga 224x224\n",
    "        \n",
    "        elif base_model.startswith('squeezenet'):\n",
    "            return (224, 224)  # SqueezeNet wymaga 224x224\n",
    "        \n",
    "        elif base_model == 'swin_t':\n",
    "            return (224, 224)  # SwinTransformer wymaga 224x224\n",
    "        \n",
    "        elif base_model == 'vit_b_16':  # VisionTransformer\n",
    "            return (224, 224)  # VisionTransformer wymaga 224x224\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported base model: {base_model}\")\n",
    "    def forward2(self, t_image, b_image, s_image):\n",
    "        # Pobierz wymagany rozmiar wejściowy\n",
    "        input_size = self.get_input_size(self.base_model)\n",
    "        \n",
    "        # Weryfikacja wejścia `t_image` i `b_image` (RGB) oraz `s_image` (binary)\n",
    "        assert t_image.shape[-2:] == input_size, f\"Expected T image to be of size {input_size}, but got {t_image.shape[-2:]}\"\n",
    "        assert b_image.shape[-2:] == input_size, f\"Expected B image to be of size {input_size}, but got {b_image.shape[-2:]}\"\n",
    "        assert s_image.shape[-2:] == input_size, f\"Expected S image to be of size {input_size}, but got {s_image.shape[-2:]}\"\n",
    "        \n",
    "        # Ekstrakcja cech dla widoków RGB\n",
    "        t_features = self.rgb_model(t_image)  # Widok T\n",
    "        b_features = self.rgb_model(b_image)  # Widok B\n",
    "\n",
    "        # Ekstrakcja cech dla obrazu binarnego\n",
    "        s_features = self.binary_model(s_image)\n",
    "\n",
    "        # Połączenie cech\n",
    "        combined_features = torch.cat([t_features, b_features, s_features], dim=1)\n",
    "\n",
    "        # Klasyfikacja\n",
    "        output = self.fc(combined_features)\n",
    "        return output\n",
    "    \n",
    "class MultiInputDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform_rgb=None, transform_binary=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "\n",
    "        # Tworzenie mapowania nazw klas na liczby całkowite\n",
    "        self.class_to_idx = {class_name: idx for idx, class_name in enumerate(self.data['class'].unique())}\n",
    "\n",
    "        self.transform_rgb = transform_rgb\n",
    "        self.transform_binary = transform_binary\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) // 3  # Każde ziarno ma 3 obrazy\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Pobierz trzy obrazy\n",
    "        base_idx = idx * 3\n",
    "        t_path = self.data.iloc[base_idx]['path']\n",
    "        b_path = self.data.iloc[base_idx + 1]['path']\n",
    "        s_path = self.data.iloc[base_idx + 2]['path']\n",
    "\n",
    "        t_image = Image.open(t_path).convert(\"RGB\")\n",
    "        b_image = Image.open(b_path).convert(\"RGB\")\n",
    "        s_image = Image.open(s_path).convert(\"L\")  # Obraz binarny\n",
    "\n",
    "        # Transformacje\n",
    "        if self.transform_rgb:\n",
    "            t_image = self.transform_rgb(t_image)\n",
    "            b_image = self.transform_rgb(b_image)\n",
    "        if self.transform_binary:\n",
    "            s_image = self.transform_binary(s_image)\n",
    "\n",
    "        # Pobierz nazwę klasy i przekształć na indeks numeryczny\n",
    "        class_name = self.data.iloc[base_idx]['class']\n",
    "        label = self.class_to_idx[class_name]  # Mapowanie nazwy klasy na numer\n",
    "        label = torch.tensor(label, dtype=torch.long)  # Konwersja na tensor PyTorch\n",
    "\n",
    "        return t_image, b_image, s_image, label\n",
    "\n",
    "#Krok 2: Transformacje dla obrazów RGB i binarnych:\n",
    "# Transformacje dla obrazów RGB\n",
    "transform_rgb = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Transformacje dla obrazów binarnych\n",
    "transform_binary = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mk/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/mk/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_V2_M_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_V2_M_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: efficientnet_v2_m, base_model_output_size: 1280\n",
      "Total input size to fc: 2688\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4344/4344 [54:35<00:00,  1.33batch/s, loss=1.9045]\n",
      "Validation: 100%|██████████| 931/931 [06:03<00:00,  2.56batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 1\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4344/4344 [54:26<00:00,  1.33batch/s, loss=1.2022]\n",
      "Validation: 100%|██████████| 931/931 [06:05<00:00,  2.55batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 2\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4344/4344 [54:25<00:00,  1.33batch/s, loss=0.7769]\n",
      "Validation: 100%|██████████| 931/931 [06:05<00:00,  2.55batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 3\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4344/4344 [54:23<00:00,  1.33batch/s, loss=0.5478]\n",
      "Validation: 100%|██████████| 931/931 [06:05<00:00,  2.55batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 4\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4344/4344 [54:25<00:00,  1.33batch/s, loss=0.4333]\n",
      "Validation: 100%|██████████| 931/931 [06:05<00:00,  2.54batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 5\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4344/4344 [54:23<00:00,  1.33batch/s, loss=0.3592]\n",
      "Validation: 100%|██████████| 931/931 [06:05<00:00,  2.55batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 6\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4344/4344 [54:24<00:00,  1.33batch/s, loss=0.3168]\n",
      "Validation: 100%|██████████| 931/931 [06:04<00:00,  2.55batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 1 epoch(s)\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4344/4344 [54:22<00:00,  1.33batch/s, loss=0.2781]\n",
      "Validation: 100%|██████████| 931/931 [06:04<00:00,  2.56batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 2 epoch(s)\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4344/4344 [54:20<00:00,  1.33batch/s, loss=0.2514]\n",
      "Validation: 100%|██████████| 931/931 [06:04<00:00,  2.55batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 3 epoch(s)\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4344/4344 [55:06<00:00,  1.31batch/s, loss=0.2297]\n",
      "Validation: 100%|██████████| 931/931 [06:04<00:00,  2.56batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 10\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4344/4344 [54:21<00:00,  1.33batch/s, loss=0.2118]\n",
      "Validation: 100%|██████████| 931/931 [06:03<00:00,  2.56batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 1 epoch(s)\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4344/4344 [54:20<00:00,  1.33batch/s, loss=0.1921]\n",
      "Validation: 100%|██████████| 931/931 [06:17<00:00,  2.47batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 2 epoch(s)\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4344/4344 [54:41<00:00,  1.32batch/s, loss=0.1831]\n",
      "Validation: 100%|██████████| 931/931 [06:05<00:00,  2.55batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 3 epoch(s)\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4344/4344 [54:20<00:00,  1.33batch/s, loss=0.1702]\n",
      "Validation: 100%|██████████| 931/931 [06:04<00:00,  2.55batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 14\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4344/4344 [54:22<00:00,  1.33batch/s, loss=0.1612]\n",
      "Validation: 100%|██████████| 931/931 [06:04<00:00,  2.55batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 1 epoch(s)\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4344/4344 [54:19<00:00,  1.33batch/s, loss=0.1582]\n",
      "Validation: 100%|██████████| 931/931 [06:04<00:00,  2.56batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 2 epoch(s)\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4344/4344 [54:21<00:00,  1.33batch/s, loss=0.1461]\n",
      "Validation: 100%|██████████| 931/931 [06:04<00:00,  2.55batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 3 epoch(s)\n",
      "Epoch 18/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4344/4344 [54:22<00:00,  1.33batch/s, loss=0.1360]\n",
      "Validation: 100%|██████████| 931/931 [06:04<00:00,  2.55batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 4 epoch(s)\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4344/4344 [54:46<00:00,  1.32batch/s, loss=0.1310] \n",
      "Validation: 100%|██████████| 931/931 [06:05<00:00,  2.55batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 5 epoch(s)\n",
      "Early stopping triggered. Training stopped.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mk/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/mk/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Small_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth\" to /home/mk/.cache/torch/hub/checkpoints/mobilenet_v3_small-047dcff4.pth\n",
      "100%|██████████| 9.83M/9.83M [00:00<00:00, 10.8MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: mobilenet_v3_small, base_model_output_size: 576\n",
      "Total input size to fc: 1280\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2172/2172 [20:54<00:00,  1.73batch/s, loss=0.4512]\n",
      "Validation: 100%|██████████| 466/466 [04:00<00:00,  1.94batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 1\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2172/2172 [20:42<00:00,  1.75batch/s, loss=0.2062]\n",
      "Validation: 100%|██████████| 466/466 [04:00<00:00,  1.94batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 2\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2172/2172 [20:44<00:00,  1.75batch/s, loss=0.1614]\n",
      "Validation: 100%|██████████| 466/466 [04:00<00:00,  1.94batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 1 epoch(s)\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2172/2172 [20:44<00:00,  1.75batch/s, loss=0.1326]\n",
      "Validation: 100%|██████████| 466/466 [04:00<00:00,  1.94batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 2 epoch(s)\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2172/2172 [20:43<00:00,  1.75batch/s, loss=0.1180]\n",
      "Validation: 100%|██████████| 466/466 [03:59<00:00,  1.94batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 3 epoch(s)\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2172/2172 [20:44<00:00,  1.74batch/s, loss=0.1047]\n",
      "Validation: 100%|██████████| 466/466 [04:00<00:00,  1.94batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 4 epoch(s)\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2172/2172 [20:43<00:00,  1.75batch/s, loss=0.0948]\n",
      "Validation: 100%|██████████| 466/466 [03:59<00:00,  1.94batch/s]\n",
      "/home/mk/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/mk/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 5 epoch(s)\n",
      "Early stopping triggered. Training stopped.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /home/mk/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n",
      "100%|██████████| 83.3M/83.3M [00:07<00:00, 11.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: resnet34, base_model_output_size: 512\n",
      "Total input size to fc: 1152\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2172/2172 [25:14<00:00,  1.43batch/s, loss=1.6726]\n",
      "Validation: 100%|██████████| 466/466 [04:24<00:00,  1.76batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 1\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2172/2172 [24:30<00:00,  1.48batch/s, loss=0.8187]\n",
      "Validation: 100%|██████████| 466/466 [04:15<00:00,  1.82batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 2\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2172/2172 [24:33<00:00,  1.47batch/s, loss=0.5215]\n",
      "Validation: 100%|██████████| 466/466 [04:15<00:00,  1.82batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 3\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2172/2172 [24:33<00:00,  1.47batch/s, loss=0.3857]\n",
      "Validation: 100%|██████████| 466/466 [04:16<00:00,  1.82batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 4\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2172/2172 [24:35<00:00,  1.47batch/s, loss=0.3127]\n",
      "Validation: 100%|██████████| 466/466 [04:15<00:00,  1.82batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 1 epoch(s)\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2172/2172 [24:34<00:00,  1.47batch/s, loss=0.2584]\n",
      "Validation: 100%|██████████| 466/466 [04:15<00:00,  1.82batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 2 epoch(s)\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2172/2172 [24:32<00:00,  1.47batch/s, loss=0.2182]\n",
      "Validation: 100%|██████████| 466/466 [04:16<00:00,  1.82batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 7\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2172/2172 [24:32<00:00,  1.47batch/s, loss=0.1866]\n",
      "Validation: 100%|██████████| 466/466 [04:16<00:00,  1.82batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 1 epoch(s)\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2172/2172 [24:35<00:00,  1.47batch/s, loss=0.1601]\n",
      "Validation: 100%|██████████| 466/466 [04:16<00:00,  1.82batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 9\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2172/2172 [24:31<00:00,  1.48batch/s, loss=0.1425]\n",
      "Validation: 100%|██████████| 466/466 [04:16<00:00,  1.82batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 10\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2172/2172 [24:31<00:00,  1.48batch/s, loss=0.1236]\n",
      "Validation: 100%|██████████| 466/466 [04:15<00:00,  1.82batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 1 epoch(s)\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2172/2172 [24:32<00:00,  1.48batch/s, loss=0.1104]\n",
      "Validation: 100%|██████████| 466/466 [04:15<00:00,  1.82batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 2 epoch(s)\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2172/2172 [24:31<00:00,  1.48batch/s, loss=0.0968]\n",
      "Validation: 100%|██████████| 466/466 [04:15<00:00,  1.82batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 3 epoch(s)\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2172/2172 [24:32<00:00,  1.47batch/s, loss=0.0890]\n",
      "Validation: 100%|██████████| 466/466 [04:15<00:00,  1.82batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 4 epoch(s)\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2172/2172 [24:32<00:00,  1.48batch/s, loss=0.0794]\n",
      "Validation: 100%|██████████| 466/466 [04:16<00:00,  1.82batch/s]\n",
      "/home/mk/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/mk/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Swin_T_Weights.IMAGENET1K_V1`. You can also use `weights=Swin_T_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 5 epoch(s)\n",
      "Early stopping triggered. Training stopped.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/swin_t-704ceda3.pth\" to /home/mk/.cache/torch/hub/checkpoints/swin_t-704ceda3.pth\n",
      "100%|██████████| 108M/108M [00:09<00:00, 11.7MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: swin_t, base_model_output_size: 768\n",
      "Total input size to fc: 1664\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4344/4344 [37:58<00:00,  1.91batch/s, loss=2.4007]\n",
      "Validation: 100%|██████████| 931/931 [05:18<00:00,  2.92batch/s]\n",
      "/home/mk/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 1\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4344/4344 [37:59<00:00,  1.91batch/s, loss=2.3969]\n",
      "/home/mk/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Validation: 100%|██████████| 931/931 [05:18<00:00,  2.93batch/s]\n",
      "/home/mk/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 2\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4344/4344 [37:57<00:00,  1.91batch/s, loss=2.3969]\n",
      "/home/mk/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Validation: 100%|██████████| 931/931 [05:17<00:00,  2.93batch/s]\n",
      "/home/mk/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement in val loss for 1 epoch(s)\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4344/4344 [37:57<00:00,  1.91batch/s, loss=2.3969]\n",
      "/home/mk/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Validation: 100%|██████████| 931/931 [05:18<00:00,  2.93batch/s]\n",
      "/home/mk/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at epoch 4\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 4344/4344 [38:00<00:00,  1.90batch/s, loss=2.3969]\n",
      "/home/mk/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Validation:  86%|████████▋ | 805/931 [04:46<00:44,  2.81batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 103\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# Wyłącz gradienty\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(val_loader), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar_val:\n\u001b[0;32m--> 103\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m            \u001b[49m\u001b[43mt_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m                \u001b[49m\u001b[43mt_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m                \u001b[49m\u001b[43mb_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m                \u001b[49m\u001b[43ms_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms_image\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[1], line 199\u001b[0m, in \u001b[0;36mMultiInputDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_rgb:\n\u001b[1;32m    198\u001b[0m     t_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_rgb(t_image)\n\u001b[0;32m--> 199\u001b[0m     b_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform_rgb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_binary:\n\u001b[1;32m    201\u001b[0m     s_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_binary(s_image)\n",
      "File \u001b[0;32m~/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/torchvision/transforms/transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/torchvision/transforms/functional.py:350\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 350\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dnn_gpu/lib/python3.11/site-packages/torchvision/transforms/_functional_tensor.py:922\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    920\u001b[0m mean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(mean, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mtensor\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    921\u001b[0m std \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(std, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mtensor\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m(\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstd evaluated to zero after conversion to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, leading to division by zero.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mean\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Włącz blokowanie błędów CUDA\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# Załaduj dane\n",
    "train_dataset = MultiInputDataset(\"CSV/dataset/train.csv\", transform_rgb=transform_rgb, transform_binary=transform_binary)\n",
    "val_dataset = MultiInputDataset(\"CSV/dataset/val.csv\", transform_rgb=transform_rgb, transform_binary=transform_binary)\n",
    "test_dataset = MultiInputDataset(\"CSV/dataset/test.csv\", transform_rgb=transform_rgb, transform_binary=transform_binary)\n",
    "\n",
    "# List of models to train\n",
    "models_list = ['efficientnet_v2_m', 'mobilenet_v3_small', 'resnet34', 'swin_t', 'vit_b_16']#'efficientnet_b0',\n",
    "batch_sizes = {\n",
    "    'efficientnet_b0': 32,\n",
    "    'mobilenet_v3_small': 32,\n",
    "    'efficientnet_v2_m': 16,\n",
    "    'resnet34': 32,\n",
    "    'swin_t' :16,\n",
    "    'vit_b_16': 8  \n",
    "}\n",
    "for model_name in models_list:\n",
    "    # Inicjalizacja modelu\n",
    "    model = MultiInputModel(num_classes=11, base_model=model_name)  # Liczba klas\n",
    "    model = model.to(\"cuda\")  # Jeśli używasz GPU\n",
    "\n",
    "    #Dynamicznie przydzielany batch_size\n",
    "\n",
    "    batch_size = batch_sizes[model_name]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Optymalizator i funkcja straty\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Zapis logów\n",
    "    log_file = f\"training_results/training_log_{model_name}.txt\"\n",
    "    with open(log_file, \"w\") as f:\n",
    "        f.write(\"epoch,train_loss,val_loss,train_accuracy,val_accuracy,train_precision,val_precision,train_recall,val_recall,train_f1,val_f1\\n\")\n",
    "\n",
    "    # Wczesne zatrzymanie - parametry\n",
    "    early_stop_patience = 5  # Liczba epok bez poprawy\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "    best_model_path = f\"training_results/best_model_{model_name}.pth\"\n",
    "\n",
    "    # Pętla treningowa\n",
    "    num_epochs = 50\n",
    "    for epoch in range(num_epochs):\n",
    "        # === TRENING ===\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_true = []\n",
    "        train_pred = []\n",
    "\n",
    "        # Dodaj pasek postępu do pętli batchy\n",
    "        with tqdm(total=len(train_loader), desc=\"Training\", unit=\"batch\") as pbar:\n",
    "            for t_image, b_image, s_image, labels in train_loader:\n",
    "                t_image, b_image, s_image, labels = (\n",
    "                    t_image.to(\"cuda\"),\n",
    "                    b_image.to(\"cuda\"),\n",
    "                    s_image.to(\"cuda\"),\n",
    "                    labels.to(\"cuda\")\n",
    "                )\n",
    "\n",
    "                # Oblicz predykcje i stratę\n",
    "                outputs = model(t_image, b_image, s_image)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                train_true.extend(labels.cpu().numpy())\n",
    "                train_pred.extend(predicted.cpu().numpy())\n",
    "                pbar.set_postfix({\"loss\": f\"{train_loss / (pbar.n + 1):.4f}\"})  # Wyświetl średnią stratę\n",
    "                pbar.update(1)  # Aktualizuj pasek postępu o 1 krok\n",
    "\n",
    "        train_loss /= len(train_loader)  # Średnia strata w treningu\n",
    "        train_accuracy = accuracy_score(train_true, train_pred)\n",
    "        train_precision = precision_score(train_true, train_pred, average=\"weighted\")\n",
    "        train_recall = recall_score(train_true, train_pred, average=\"weighted\")\n",
    "        train_f1 = f1_score(train_true, train_pred, average=\"weighted\")\n",
    "        #print(f\"Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        # === WALIDACJA ===\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_true = []\n",
    "        val_pred = []\n",
    "\n",
    "        with torch.no_grad():  # Wyłącz gradienty\n",
    "            with tqdm(total=len(val_loader), desc=\"Validation\", unit=\"batch\") as pbar_val:\n",
    "                for t_image, b_image, s_image, labels in val_loader:\n",
    "                    t_image, b_image, s_image, labels = (\n",
    "                        t_image.to(\"cuda\"),\n",
    "                        b_image.to(\"cuda\"),\n",
    "                        s_image.to(\"cuda\"),\n",
    "                        labels.to(\"cuda\")\n",
    "                    )\n",
    "                    outputs = model(t_image, b_image, s_image)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    val_true.extend(labels.cpu().numpy())\n",
    "                    val_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "                    pbar_val.update(1)  # Aktualizuj pasek postępu walidacji\n",
    "\n",
    "        val_loss /= len(val_loader)  # Średnia strata w walidacji\n",
    "        val_accuracy = accuracy_score(val_true, val_pred)\n",
    "        val_precision = precision_score(val_true, val_pred, average=\"weighted\")\n",
    "        val_recall = recall_score(val_true, val_pred, average=\"weighted\")\n",
    "        val_f1 = f1_score(val_true, val_pred, average=\"weighted\")\n",
    "        #print(f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # === LOGI ===\n",
    "        #print(f\"Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        with open(log_file, \"a\") as f:\n",
    "            f.write(\n",
    "                f\"{epoch + 1},{train_loss:.4f},{val_loss:.4f},{train_accuracy:.4f},{val_accuracy:.4f},\"\n",
    "                f\"{train_precision:.4f},{val_precision:.4f},{train_recall:.4f},{val_recall:.4f},{train_f1:.4f},{val_f1:.4f}\\n\"\n",
    "            )\n",
    "\n",
    "        # === WCZESNE ZATRZYMANIE ===\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            # Zapis najlepszego modelu\n",
    "            torch.save(model, best_model_path) #Zapisanie modelu i architektury w pliku pth\n",
    "            print(f\"Best model saved at epoch {epoch + 1}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement in val loss for {patience_counter} epoch(s)\")\n",
    "\n",
    "        if patience_counter >= early_stop_patience:\n",
    "            print(\"Early stopping triggered. Training stopped.\")\n",
    "            break\n",
    "    \"\"\"\n",
    "    # === TEST ===\n",
    "    # Wczytaj najlepszy model\n",
    "    model.load(torch.load(best_model_path))\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for t_image, b_image, s_image, labels in test_loader:\n",
    "            t_image, b_image, s_image, labels = (\n",
    "                t_image.to(\"cuda\"),\n",
    "                b_image.to(\"cuda\"),\n",
    "                s_image.to(\"cuda\"),\n",
    "                labels.to(\"cuda\")\n",
    "            )\n",
    "            outputs = model(t_image, b_image, s_image)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Oblicz dokładność\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    # Zapis wyniku testu\n",
    "    log_file_test = f\"training_results/test_log_{model_name}.txt\"\n",
    "    with open(log_file_test, \"w\") as f_t:\n",
    "            f_t.write(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
    "    \"\"\"\n",
    "    # Zwalnianie pamięci po zakończeniu pracy z modelem\n",
    "    del model  # Usuń model z pamięci\n",
    "    torch.cuda.empty_cache()  # Wyczyść pamięć GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TEST ===\n",
    "# Wczytaj najlepszy model\n",
    "model = torch.load(best_model_path)\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for t_image, b_image, s_image, labels in test_loader:\n",
    "        t_image, b_image, s_image, labels = (\n",
    "            t_image.to(\"cuda\"),\n",
    "            b_image.to(\"cuda\"),\n",
    "            s_image.to(\"cuda\"),\n",
    "            labels.to(\"cuda\")\n",
    "        )\n",
    "        outputs = model(t_image, b_image, s_image)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Oblicz dokładność\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "accuracy = correct / total\n",
    "\n",
    "# Zapis wyniku testu\n",
    "log_file_test = f\"training_results/test_log_{model_name}.txt\"\n",
    "with open(log_file_test, \"w\") as f_t:\n",
    "        f_t.write(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Zwalnianie pamięci po zakończeniu pracy z modelem\n",
    "del model  # Usuń model z pamięci\n",
    "torch.cuda.empty_cache()  # Wyczyść pamięć GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*******************"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnn_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
